{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import trainer\n",
    "#import data_loader\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from collections import Counter\n",
    "from konlpy.tag import Mecab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, train_path, test_path):\n",
    "        self.train_data = pd.read_table('~/aiffel/sentiment_classification/data/ratings_train.txt')\n",
    "        self.test_data = pd.read_table('~/aiffel/sentiment_classification/data/ratings_test.txt')\n",
    "\n",
    "    def load_data(self, num_words=10000):\n",
    "        tokenizer = Mecab()\n",
    "        stopwords = ['의', '가', '이', '은', '들', '는', '좀', '잘', '걍', '과', '도', '를', '으로', '자', '에', '와', '한', '하다']\n",
    "        self.train_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "        self.train_data = self.train_data.dropna(how='any')\n",
    "        self.test_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "        self.test_data = self.test_data.dropna(how='any')\n",
    "\n",
    "        X_train = []\n",
    "        for sentence in self.train_data['document']:\n",
    "            temp_X = tokenizer.morphs(sentence)  # 토큰화\n",
    "            temp_X = [word for word in temp_X if not word in stopwords]  # 불용어 제거\n",
    "            X_train.append(temp_X)\n",
    "\n",
    "        X_test = []\n",
    "        for sentence in self.test_data['document']:\n",
    "            temp_X = tokenizer.morphs(sentence)  # 토큰화\n",
    "            temp_X = [word for word in temp_X if not word in stopwords]  # 불용어 제거\n",
    "            X_test.append(temp_X)\n",
    "\n",
    "        words = np.concatenate(X_train).tolist()\n",
    "        counter = Counter(words)\n",
    "        counter = counter.most_common(num_words - 4)\n",
    "        vocab = ['<PAD>', '<BOS>', '<UNK>', '<UNUSED>'] + [key for key, _ in counter]\n",
    "        self.word_to_index = {word: index for index, word in enumerate(vocab)}\n",
    "\n",
    "        def wordlist_to_indexlist(wordlist):\n",
    "            return [self.word_to_index[word] if word in self.word_to_index else self.word_to_index['<UNK>'] for word in wordlist]\n",
    "\n",
    "        self.X_train = list(map(wordlist_to_indexlist, X_train))\n",
    "        self.X_test = list(map(wordlist_to_indexlist, X_test))\n",
    "        self.Y_train = np.array(list(self.train_data['label']))\n",
    "        self.Y_test = np.array(list(self.test_data['label']))\n",
    "        #return self.X_train, self.Y_train, self.X_test, self.Y_test, self.word_to_index\n",
    "\n",
    "    def get_maxlen(self):\n",
    "        total_data_text = list(self.X_train) + list(self.X_test)\n",
    "        # 텍스트데이터 문장길이의 리스트를 생성한 후\n",
    "        num_tokens = [len(tokens) for tokens in total_data_text]\n",
    "        num_tokens = np.array(num_tokens)\n",
    "        # 문장길이의 평균값, 최대값, 표준편차를 계산해 본다.\n",
    "        print('문장길이 평균 : ', np.mean(num_tokens))\n",
    "        print('문장길이 최대 : ', np.max(num_tokens))\n",
    "        print('문장길이 표준편차 : ', np.std(num_tokens))\n",
    "\n",
    "        # 예를들어, 최대 길이를 (평균 + 2*표준편차)로 한다면,\n",
    "        max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "        self.maxlen = int(max_tokens)\n",
    "        print('pad_sequences maxlen : ', self.maxlen)\n",
    "        print('전체 문장의 {}%가 maxlen 설정값 이내에 포함됩니다. '.format(np.sum(num_tokens < max_tokens) / len(num_tokens)))\n",
    "        return self.maxlen\n",
    "\n",
    "    def set_pad(self, padding='post'):\n",
    "        self.X_train = keras.preprocessing.sequence.pad_sequences(self.X_train,\n",
    "                                                             value=self.word_to_index[\"<PAD>\"],\n",
    "                                                             padding='post',  # 혹은 'pre'\n",
    "                                                             maxlen=self.maxlen)\n",
    "\n",
    "        self.X_test = keras.preprocessing.sequence.pad_sequences(self.X_test,\n",
    "                                                            value=self.word_to_index[\"<PAD>\"],\n",
    "                                                            padding='post',  # 혹은 'pre'\n",
    "                                                            maxlen=self.maxlen)\n",
    "        return self.X_train, self.X_test\n",
    "\n",
    "    def get_data(self, train_idx=30000):\n",
    "        x_train = self.X_train[:train_idx]\n",
    "        x_val = self.X_train[train_idx:]\n",
    "\n",
    "        y_train = self.Y_train[:train_idx]\n",
    "        y_val = self.Y_train[train_idx:]\n",
    "\n",
    "        return x_train, y_train, x_val, y_val, self.X_test, self.Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Kernel is dead",
     "output_type": "error",
     "traceback": [
      "Error: Kernel is dead",
      "at f._sendKernelShellControl (/home/estela19/.vscode-server/extensions/ms-toolsai.jupyter-2021.9.1101343141/out/client/node_modules/@jupyterlab/services.js:3:431671)",
      "at f.sendShellMessage (/home/estela19/.vscode-server/extensions/ms-toolsai.jupyter-2021.9.1101343141/out/client/node_modules/@jupyterlab/services.js:3:431440)",
      "at f.requestExecute (/home/estela19/.vscode-server/extensions/ms-toolsai.jupyter-2021.9.1101343141/out/client/node_modules/@jupyterlab/services.js:3:433992)",
      "at _.requestExecute (/home/estela19/.vscode-server/extensions/ms-toolsai.jupyter-2021.9.1101343141/out/client/extension.js:32:19306)",
      "at w.executeCodeCell (/home/estela19/.vscode-server/extensions/ms-toolsai.jupyter-2021.9.1101343141/out/client/extension.js:52:300924)",
      "at w.execute (/home/estela19/.vscode-server/extensions/ms-toolsai.jupyter-2021.9.1101343141/out/client/extension.js:52:300551)",
      "at w.start (/home/estela19/.vscode-server/extensions/ms-toolsai.jupyter-2021.9.1101343141/out/client/extension.js:52:296215)",
      "at runMicrotasks (<anonymous>)",
      "at processTicksAndRejections (internal/process/task_queues.js:93:5)",
      "at async t.CellExecutionQueue.executeQueuedCells (/home/estela19/.vscode-server/extensions/ms-toolsai.jupyter-2021.9.1101343141/out/client/extension.js:52:310950)",
      "at async t.CellExecutionQueue.start (/home/estela19/.vscode-server/extensions/ms-toolsai.jupyter-2021.9.1101343141/out/client/extension.js:52:310490)"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class Trainer():\n",
    "    def data_split(self, src_data, tgt_data,  val_size = 0.2, test_size = 0.2):\n",
    "        x_data, self.test_x, y_data, self.test_y = train_test_split(src_data, tgt_data, test_size=test_size)\n",
    "        self.train_x, self.val_x, self.train_y, self.val_y = train_test_split(x_data, y_data, test_size=val_size)\n",
    "\n",
    "    def train(self, model, optimizer, loss, epochs=100, batch_size=512, verbose=2):\n",
    "        model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "        self.hist = model.fit(self.train_x, self.train_y, epochs=epochs, batch_size=batch_size, validation_data=(self.val_x, self.val_y), verbose=verbose)\n",
    "\n",
    "    def test(self, model):\n",
    "        #result = model.evaluate(self.test_x, self.test_y, verbose=2)\n",
    "        #print(result)\n",
    "        test_loss, test_acc = self.model.evaluate(self.test_x, self.test_y)\n",
    "        print(\"test_loss    :{}\".format(test_loss))\n",
    "        print(\"test_accuracy:{}\".format(test_acc))\n",
    "\n",
    "    def visualization(self):\n",
    "        history_dict = self.hist.history\n",
    "        \n",
    "        acc = history_dict['accuracy']\n",
    "        val_acc = history_dict['val_accuracy']\n",
    "        loss = history_dict['loss']\n",
    "        val_loss = history_dict['val_loss']\n",
    "\n",
    "        epochs = range(1, len(acc) + 1)\n",
    "\n",
    "        plt.figure(figsize=(12, 8))\n",
    "\n",
    "        # loss 그래프\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "        plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "        plt.title('Training and validation loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "\n",
    "        # accuracy 그래프\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(epochs, acc, 'b', label='Training acc')\n",
    "        plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "        plt.title('Training and validation accuracy')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_path = '~/aiffel/sentiment_classification/data/ratings_train.txt'\n",
    "test_path = '~/aiffel/sentiment_classification/data/ratings_test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_loader = DataLoader(train_path, test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장길이 평균 :  15.96940191154864\n",
      "문장길이 최대 :  116\n",
      "문장길이 표준편차 :  12.843571191092\n",
      "pad_sequences maxlen :  41\n",
      "전체 문장의 0.9342988343341575%가 maxlen 설정값 이내에 포함됩니다. \n"
     ]
    }
   ],
   "source": [
    "data_loader.load_data(10000)\n",
    "data_loader.get_maxlen()\n",
    "data_loader.set_pad('post')\n",
    "x_train, y_train , x_val, y_val, x_test, y_test = data_loader.get_data(train_idx=30000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Base Model\n",
    "LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 25)          250000    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 512)               1101824   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,680,273\n",
      "Trainable params: 1,680,273\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 25\n",
    "\n",
    "lstm_model = keras.Sequential()\n",
    "lstm_model.add(keras.layers.Embedding(vocab_size, word_vector_dim))\n",
    "lstm_model.add(keras.layers.LSTM(512, activation = 'relu'))\n",
    "lstm_model.add(keras.layers.Dense(512, activation='relu'))\n",
    "lstm_model.add(keras.layers.Dense(128, activation='relu'))\n",
    "lstm_model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Attention model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 41)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 41)           1722        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, 41)           0           input_3[0][0]                    \n",
      "                                                                 dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 1)            42          multiply_2[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 1,764\n",
      "Trainable params: 1,764\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_dim = 41\n",
    "\n",
    "# input layer\n",
    "inputs = keras.layers.Input(shape=(input_dim,))\n",
    "\n",
    "# attention layer\n",
    "attention_probs = keras.layers.Dense(input_dim, activation='softmax')(inputs)\n",
    "attention_mul = keras.layers.multiply([inputs, attention_probs])\n",
    "\n",
    "# fc layer\n",
    "fc = keras.layers.Dense(64)(attention_mul)\n",
    "output = keras.layers.Dense(1, activation='sigmoid')(attention_mul)\n",
    "\n",
    "attention_model = keras.Model(inputs=[inputs], outputs=output)\n",
    "\n",
    "attention_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "trainer = Trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "trainer.train_x = x_train\n",
    "trainer.val_x = x_val\n",
    "trainer.test_x = x_test\n",
    "trainer.train_y = y_train\n",
    "trainer.val_y = y_val\n",
    "trainer.test_y = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "59/59 - 1s - loss: 0.7634 - val_loss: 0.7441\n",
      "Epoch 2/10\n",
      "59/59 - 0s - loss: 0.7118 - val_loss: 0.7331\n",
      "Epoch 3/10\n",
      "59/59 - 0s - loss: 0.7040 - val_loss: 0.7348\n",
      "Epoch 4/10\n",
      "59/59 - 0s - loss: 0.7022 - val_loss: 0.7147\n",
      "Epoch 5/10\n",
      "59/59 - 0s - loss: 0.7007 - val_loss: 0.7255\n",
      "Epoch 6/10\n",
      "59/59 - 0s - loss: 0.7041 - val_loss: 0.7391\n",
      "Epoch 7/10\n",
      "59/59 - 0s - loss: 0.7205 - val_loss: 0.7530\n",
      "Epoch 8/10\n",
      "59/59 - 0s - loss: 0.7079 - val_loss: 0.7287\n",
      "Epoch 9/10\n",
      "59/59 - 0s - loss: 0.7054 - val_loss: 0.7291\n",
      "Epoch 10/10\n",
      "59/59 - 0s - loss: 0.7027 - val_loss: 0.7415\n"
     ]
    }
   ],
   "source": [
    "trainer.train(model=attention_model, optimizer='adam', loss='binary_crossentropy', epochs=10, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-55-324ca3154f1b>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mtrainer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvisualization\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m<ipython-input-51-8b37e2f278bf>\u001B[0m in \u001B[0;36mvisualization\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     21\u001B[0m         \u001B[0mhistory_dict\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhist\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhistory\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 23\u001B[0;31m         \u001B[0macc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhist\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhistory\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'accuracy'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     24\u001B[0m         \u001B[0mval_acc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mhistory_dict\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'val_accuracy'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     25\u001B[0m         \u001B[0mloss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mhistory_dict\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'loss'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: 'accuracy'"
     ]
    }
   ],
   "source": [
    "trainer.visualization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1537/1537 - 2s - loss: 0.7302\n",
      "0.7302318811416626\n"
     ]
    }
   ],
   "source": [
    "trainer.test(attention_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "trainer.test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}