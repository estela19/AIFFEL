# Sentiment Classification  
ë„¤ì´ë²„ ì˜í™” ë¦¬ë·° ë°ì´í„°ì…‹ì„ ì´ìš©í•´ ì˜í™” ë¦¬ë·°ë¥¼ ë³´ê³  ê¸ì •ì ì¸ ë¦¬ë·°ì¸ì§€, ë¶€ì •ì ì¸ ë¦¬ë·°ì¸ì§€ Binary Classificationí•˜ëŠ” ëª¨ë¸ì„ ë§Œë“¤ì–´ ë³´ì!  
[ğŸ‘‰ğŸ‘‰Go To CodeğŸ‘ˆğŸ‘ˆ](https://github.com/estela19/AIFFEL/blob/master/exp07/models.ipynb)  

# Workflow  
### 1. DataLoader 
### 2. Add padding
### 3. Modeling  
### 4. Model Train  
### 5. Model Test  

# Embedding  
ì¼ë°˜ì ìœ¼ë¡œ pretrain ëœ embeddingë³´ë‹¤ ì›í•« embeddingì´ ì£¼ì–´ì§„ taskì— ë§ê²Œ ë”ìš± ì˜ embedding ë˜ê¸° ë•Œë¬¸ì— ì¼ë°˜ì ìœ¼ë¡œ pretrain ì„ë² ë”©ì„ ì˜ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤.  
ë‹¤ë§Œ, í•™ìŠµë°ì´í„°ê°€ ì‘ì„ ê²½ìš° ë¯¸ë¦¬ ë§ì€ ë°ì´í„°ë¡œ í•™ìŠµí•œ pretrain embeddingì´ ë” íš¨ê³¼ì ì´ë‹¤.  
ë³¸ ëª¨ë¸ì—ì„œ ì‚¬ìš©í•œ embeddingì€ keras embedding layerì™€ word2vecì´ë‹¤.  

## keras embedding layer  
kearasì—ì„œ ì œê³µí•˜ëŠ” embedding layerë¡œ ê¸°ë³¸ì ìœ¼ë¡œ one-hot embeddingì„ ì´ìš©í•œë‹¤.  

## word2vec 
cbowì™€ skip gram í˜•ì‹ìœ¼ë¡œ í•™ìŠµí•œ ì¸ë² ë”©ìœ¼ë¡œ ì˜¤í† ì¸ì½”ë”ì™€ ë¹„ìŠ·í•œ ì„±ê²©ì„ ê°€ì¡Œë‹¤.  

### ì¥ì   
  * ë¹ ë¥´ë‹¤
  * ë¹„êµì  ì •í™•í•œ ë²¡í„°
  
### ë‹¨ì   
  * ì¶œí˜„ë¹ˆë„ê°€ ì ì€ ë‹¨ì–´ì¼ ê²½ìš° ë²¡í„°ê°€ ì •í™•í•˜ì§€ ì•Šë‹¤.

## fasttext  
ë¹ˆë„ê°€ ì ì€ ë‹¨ì–´ì— ëŒ€í•œ í•™ìŠµê³¼ OoV(out of vocabulary)ì— ëŒ€í•œ ëŒ€ì²˜ê°€ ì–´ë ¤ìš´ word2vecë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´ ë§Œë“¤ì–´ì¡Œë‹¤.  
ì´ë¥¼ ìœ„í•´ subwordë¡œ ë¶„ì ˆí•œ í›„ ì›ë˜ ë‹¨ì–´ì˜ embeddingì„ êµ¬í•˜ëŠ” ë°©ì‹ì„ íƒí–ˆë‹¤.  

### í•™ìŠµë°©ë²•  
1. ë‹¨ì–´ë¥¼ subwordë¡œ ë‚˜ëˆˆë‹¤.
2. skip-gramì„ í™œìš©, ê° subwordì— ëŒ€í•œ embedding vectorì— ì£¼ë³€ë‹¨ì–´ì˜ context vectorë¥¼ ê³±í•œë‹¤ .  
3. ìœ„ ê°’ì´ ìµœëŒ€ê°€ ë˜ë„ë¡ í•™ìŠµí•œë‹¤.  
   

# Model  
## LSTM  
í•˜ë‚˜ì˜ lstm layer ì™€ dense layerë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤.  
```python
vocab_size = 10000    
word_vector_dim = 25

lstm_model = keras.Sequential()
lstm_model.add(keras.layers.Embedding(vocab_size, word_vector_dim))
lstm_model.add(keras.layers.LSTM(256, activation = 'relu'))
lstm_model.add(keras.layers.Dense(256, activation='relu'))
lstm_model.add(keras.layers.Dense(128, activation='relu'))
lstm_model.add(keras.layers.Dense(1, activation='sigmoid'))
```  

## Attention before LSTM  
attention layerë¥¼ í†µê³¼í•œ í›„ lstm ì—°ì‚°ì„ ì‹œí–‰í•œë‹¤.  
attention ë°©ì‹ì—ëŠ” ë‹¤ì–‘í•œ ë°©ë²•ì´ ìˆì§€ë§Œ `embedding ëœ input tensor`ì™€ ì´ `inputì„ dense layerì— í†µê³¼ì‹œí‚¨ tensor`ì˜ `dot product`ë¥¼ í†µí•´ì„œ êµ¬í˜„í•˜ì˜€ë‹¤.  

```python
vocab_size = 10000   
word_vector_dim = 25
time_steps = 41

# input layer
inputs = keras.layers.Input(shape=(time_steps,))

embed = keras.layers.Embedding(vocab_size, word_vector_dim)(inputs)

# attention layer
a = keras.layers.Permute((2, 1))(embed) 

a = keras.layers.Dense(time_steps, activation='softmax')(a)
a_probs = keras.layers.Permute((2, 1), name='attention_vec')(a)
output_attention_mul  = keras.layers.multiply([embed, a_probs])


# lstm layer
lstm_out = keras.layers.LSTM(256, return_sequences=True, activation='relu')(output_attention_mul)

# fc layer
fc = keras.layers.Dense(64)(lstm_out)
output = keras.layers.Dense(1, activation='sigmoid')(fc)

attention_3d_model = keras.Model(inputs=[inputs], outputs=output)
```  


## Attention after LSTM  
lstm layerë¥¼ í†µê³¼í•œ í›„ attention ì—°ì‚°ì„ ìˆ˜í–‰í•œë‹¤.  
attentionì€ lstmì˜ ë³´ì¡°ì ì¸ ì—°ì‚°ì´ë¼ê³  ìƒê°í•´ lstm layerë¥¼ ê±°ì¹œ í›„ attentionì„ í•˜ëŠ” ê²ƒì´ í° ì˜ë¯¸ê°€ ì—†ì„ ê²ƒìœ¼ë¡œ ìƒê°í–ˆìœ¼ë‚˜  ì˜ì™¸ë¡œ lstm layerë¥¼ ê±°ì¹˜ê¸° ì „ attentionì„ í•˜ëŠ” ê²ƒê³¼ ë¹„ìŠ·í•œ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.  

```python
vocab_size = 10000   
word_vector_dim = 25
time_steps = 41

# input layer
inputs = keras.layers.Input(shape=(time_steps,))

embed = keras.layers.Embedding(vocab_size, word_vector_dim)(inputs)

# lstm layer
lstm_out = keras.layers.LSTM(256, return_sequences=True, activation='relu')(embed)

# attention layer
a = keras.layers.Permute((2, 1))(lstm_out)
a = keras.layers.Dense(time_steps, activation='softmax')(a)
a_probs = keras.layers.Permute((2, 1), name='attention_vec')(a)
attention_mul = keras.layers.multiply([lstm_out, a_probs])

# fc layer
fc = keras.layers.Dense(64)(attention_mul)
output = keras.layers.Dense(1, activation='sigmoid')(fc)

Att_after_LSTM = keras.Model(inputs=[inputs], outputs=output)

Att_after_LSTM.summary()
```

## Transformer
transformerëŠ” attentionë§Œìœ¼ë¡œ ì¸ì½”ë”ì™€ ë””ì½”ë”ë¥¼ êµ¬í˜„í•œ ëª¨ë¸ì´ë‹¤. 
ë³¸ ëª¨ë¸ì— ëŒ€í•œ ì´í•´ê°€ ì•„ì§ì€ ì™„ë²½í•˜ì§€ ì•Šì•„ ì´ë¯¸ êµ¬í˜„ë˜ì–´ ìˆëŠ” ì½”ë“œë¥¼ ì´ìš©í•´ ëª¨ë¸ë§í–ˆë‹¤.  
```python
embedding_dim = 32  # ê° ë‹¨ì–´ì˜ ì„ë² ë”© ë²¡í„°ì˜ ì°¨ì›
num_heads = 2  # ì–´í…ì…˜ í—¤ë“œì˜ ìˆ˜
dff = 32  # í¬ì§€ì…˜ ì™€ì´ì¦ˆ í”¼ë“œ í¬ì›Œë“œ ì‹ ê²½ë§ì˜ ì€ë‹‰ì¸µì˜ í¬ê¸°
vocab_size = 10000
max_len = 41

inputs = keras.layers.Input(shape=(max_len,))
embedding_layer = TokenAndPositionEmbedding(max_len, vocab_size, embedding_dim)
x = embedding_layer(inputs)
transformer_block = TransformerBlock(embedding_dim, num_heads, dff)
x = transformer_block(x)
x = keras.layers.GlobalAveragePooling1D()(x)
x = keras.layers.Dropout(0.1)(x)
x = keras.layers.Dense(20, activation="relu")(x)
x = keras.layers.Dropout(0.1)(x)
outputs = keras.layers.Dense(1, activation="sigmoid")(x)

transformer = keras.Model(inputs=inputs, outputs=outputs)
```  
Transformerì˜ ì„±ëŠ¥ì€ ì´ë¯¸ ì—¬ëŸ¬ì°¨ë¡€ ê²€ì¦ì´ ë˜ì–´ìˆê¸°ì— ê°€ì‹œì ì¸ ì„±ëŠ¥ í–¥ìƒì„ ê¸°ëŒ€í–ˆìœ¼ë‚˜ ì˜ì™¸ë¡œ ë‹¨ì¼ attentionëª¨ë¸ê³¼ accuracyê°€ í¬ê²Œ ë‹¤ë¥´ì§€ ì•Šì•˜ë‹¤.  ë˜í•œ valid accuracyê°€ ë‹¤ë¥¸ ëª¨ë¸ê³¼ ë‹¬ë¦¬ 1 epochë§Œì— 0.84ë¥¼ ê¸°ë¡í–ˆì§€ë§Œ í•™ìŠµì„ ì§„í–‰í•´ë„ í¬ê²Œ ìˆ˜ë ´í•˜ì§€ ì•ŠëŠ” ëª¨ìŠµì„ ë³´ì˜€ìœ¼ë©° ì˜¤íˆë ¤ accuracyê°€ ê°ì†Œí•˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆì—ˆë‹¤.  transformerë¥¼ ì ì ˆíˆ í•™ìŠµì‹œí‚¤ê¸°ì— ë³¸ ë°ì´í„°ì…‹ì´ ì‘ê¸° ë•Œë¬¸ì´ë¼ê³  ìœ ì¶”í–ˆë‹¤.  

## Self Attention  
Attentionì˜ ì„¸ê°€ì§€ ìš”ì†Œì¸ key, query, valueê°€ ëª¨ë‘ ê°™ì€ attentionì„ Self attention ì´ë¼ê³  í•œë‹¤.  
self attentionì€ `keras_self_attention`ì„ í†µí•´ì„œ ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.  
```python
SelfAttention = keras.Sequential()
SelfAttention.add(keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_len))
SelfAttention.add(keras.layers.LSTM(256, return_sequences=True, activation='relu'))
SelfAttention.add(SeqSelfAttention(attention_activation='sigmoid'))
SelfAttention.add(keras.layers.Dense(1, activation='sigmoid'))
```


# Result  
## Default Embedding  

| model                 | hidden  size | embedding  size | accuracy |
|-----------------------|--------------|-----------------|----------|
| LSTM                  | 256          | 25              | 0.804    |
| Attention before LSTM | 256          | 25              | 0.8399   |
| Attention after LSTM  | 256          | 25              | 0.8377   |
| Transformer           | -            | 32              | 0.8439   |
| Self Attention        | 256          | 32              | 0.8504   |  

## Word2Vec Embedding
| model                 | hidden  size | embedding  size | accuracy |
|-----------------------|--------------|-----------------|----------|
| LSTM                  | 128          | 200             | 0.8174   |
| Attention before LSTM | 256          | 200             | 0.7828   |
| Attention after LSTM  | 256          | 200             | 0.6993   |
| Self Attention        | 256          | 200             | 0.8193   | 

# Appendix  - Pororo
ì¹´ì¹´ì˜¤ë¸Œë ˆì¸ì—ì„œ Bert, Transformerë“±ì„ ì´ìš©í•´ ë§Œë“  ìì—°ì–´ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ [pororo](github.com/kakaobrain/pororo)ë¥¼ ì´ìš©í•˜ì—¬ í…ŒìŠ¤íŠ¸í•´ ë³´ì•˜ë‹¤.  
[ğŸ‘‰ğŸ‘‰Go To CodeğŸ‘ˆğŸ‘ˆ](https://github.com/estela19/AIFFEL/blob/master/exp07/pororo.ipynb)  
pororo ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ì‚¬ìš©ë°©ë²•ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.  
```
pip install pororo
```
```python
from pororo import Pororo

movie = Pororo(task='sentiment', model='brainbert.base.ko.nsmc', lang='ko')

movie("ë¹„ì£¼ì–¼ë¡œ ì„¤ë“ì‹œí‚¤ëŠ” ëŒ€ì„œì‚¬ì‹œ.")
```  
ìœ„ì™€ ê°™ì´ pororo ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í–ˆì„ë•Œ `Negative` ë˜ëŠ” `Positive`ë¥¼ ë¦¬í„´í•´ì¤€ë‹¤.  
í•´ë‹¹ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ë„¤ì´ë²„ ì˜í™”ë¦¬ë·° test datasetì„ ì‚¬ìš©í–ˆì„ë•Œ **0.9103**ìœ¼ë¡œ ë§¤ìš° ë†’ì€ ì •í™•ë„ë¥¼ ë³´ì—¬ì£¼ì—ˆë‹¤.   

# Think about..
* ì§ì ‘ êµ¬í˜„í•œ attentionì—ì„œ paddingì— ì§‘ì¤‘í•˜ëŠ” ë“± ì˜ëª»ëœ attention vector ì¡´ì¬í–ˆëŠ”ë° ì–´ë–»ê²Œ ê°œì„ í• ê¹Œ?  
* attentioní• ë•Œ BOS, EOS, UNK, PAD ë“±ì˜ í† í°ì€ ì–´ë–»ê²Œ ì²˜ë¦¬í•´ì£¼ì–´ì•¼ í• ê¹Œ?

# Todo
* ë°ì´í„° ìì²´ë¥¼ ì •ì œ í›„ ì •í™•ë„ ë¹„êµ
* CNNì„ ì´ìš©í•œ text classification
* train dataë¥¼ ê¸¸ì´ë¡œ sorting í•œ í›„ batchë§ˆë‹¤ ê°€ì¥ ê¸´ ë¬¸ì¥ ê¸°ì¤€ìœ¼ë¡œ paddingì„ ì±„ìš°ê¸° (paddingì´ ìƒëŒ€ì ìœ¼ë¡œ ì ê²Œ ë“¤ì–´ê°€ì„œ í•™ìŠµì˜ íš¨ìœ¨ì„ ë†’ì¼ ìˆ˜ ìˆìŒ)  

# Reference
[ë°‘ë°”ë‹¥ë¶€í„° ì´í•´í•˜ëŠ” ì–´í…ì…˜ ë§¤í„°ë‹ˆì¦˜](https://glee1228.tistory.com/3)  
[ì¼€ë¼ìŠ¤ ì–´í…ì…˜ ë§¤ì»¤ë‹ˆì¦˜](https://yjam.tistory.com/73?category=1080798)  
[ì–´í…ì…˜ê³¼ íŠ¸ëœìŠ¤í¬ë¨¸](https://huidea.tistory.com/150)  
[ë”¥ëŸ¬ë‹ì„ ì´ìš©í•œ ìì—°ì–´ ì²˜ë¦¬ ì…ë¬¸](https://wikidocs.net/103802)